{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import inflect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re, string, unicodedata\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "requisicao = [\"\"\"HTTP/1.1 200 OK\n",
    "Cache-Control: private\n",
    "Content-Length: 609\n",
    "Content-Type: text/html\n",
    "X-Powered-By: ASP.NET\n",
    "Date: Mon, 03 Dec 2018 14:02:42 GMT\n",
    "Connection: close\n",
    "\n",
    "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\" />\n",
    "<meta http-equiv=\"Content-Language\" content=\"pt\" />\n",
    "<meta http-equiv=\"Expires\" content=\"0\" />\n",
    "<meta http-equiv=\"Cache-Control\" content=\"no-cache, must-revalidate\" />\n",
    "<meta http-equiv=\"Pragma\" content=\"no-cache\" />\n",
    "\n",
    "<script>\n",
    "</script>\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = [\"\"\"POST example.asp HTTP/1.1\n",
    "Host: example.com.br\n",
    "Connection: close\n",
    "Content-Length: 175\n",
    "Cache-Control: max-age=0\n",
    "Origin: https://example\n",
    "Upgrade-Insecure-Requests: 1\n",
    "Content-Type: application/x-www-form-urlencoded\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36\n",
    "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\n",
    "Referer: https:example\n",
    "Accept-Encoding: gzip, deflate\n",
    "Accept-Language: pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\n",
    "Cookie: 14514362356\n",
    "txtOptSel=i76568676776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_req = word_tokenize(str(requisicao))\n",
    "tokenized_res = word_tokenize(str(resposta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words): # Converte minusculo\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words): # Remove pontuação\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words): # Replace all interger occurrences with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words): # Remove stopwords\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = normalize(tokenized_req)\n",
    "res = normalize(tokenized_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vec.fit_transform(req)\n",
    "req = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = vec.fit_transform(res)\n",
    "res = cosine_similarity(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity (v, w):\n",
    "    return np.dot(v, w)/np.sqrt(np.dot(v,v)*np.dot(w,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(iid, features=res, text=res, n_text=1):\n",
    "    similarity = [[cosine_similarity(features[iid],feature), int(i)] for i, feature in enumerate(features)]\n",
    "    similarity = np.array(sorted(similarity, key=lambda sim: sim[0], reverse=True))\n",
    "    return [[text[y], similarity[x, 0]] for x, y in enumerate(np.int0(similarity[1:,1]), 1)][:n_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('texto',requisicao[0])\n",
    "for t, s in text_similarity(0):\n",
    "    print('S:{} '.format(round(s, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FÓRMULA MATEMÁTICA:\n",
    "import math\n",
    "3/(math.sqrt(7)*math.sqrt(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count ocorrencia de palavras\n",
    "a_vals = Counter(tokenized_req)\n",
    "b_vals = Counter(tokenized_res)\n",
    "\n",
    "# C0nvertendo word-vectors\n",
    "words  = list(a_vals.keys() | b_vals.keys())\n",
    "a_vect = [a_vals.get(word, 0) for word in words]\n",
    "b_vect = [b_vals.get(word, 0) for word in words]\n",
    "\n",
    "# COsseno\n",
    "len_a  = sum(av*av for av in a_vect) ** 0.5\n",
    "len_b  = sum(bv*bv for bv in b_vect) ** 0.5\n",
    "dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))\n",
    "cosine = dot / (len_a * len_b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = np.dot(a_vect, b_vect) / np.sqrt(np.dot(a_vect, a_vect) * np.dot(b_vect, b_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
