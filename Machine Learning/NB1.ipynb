{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse exercício pode ser feito em grupo de até 3 pessoas.\n",
    "\n",
    "Escreva um chatbot que, dado uma pergunta em Inglês, encontre uma pergunta mais parecida no corpus de perguntas e respostas disponível no Kaggle (https://www.kaggle.com/rtatman/questionanswer-dataset#S08_question_answer_pairs.txt) e exiba a resposta.\n",
    "\n",
    "Resolva usando o Kaggle e somente compartilhe com fernandojvdasilva e envie o link na hora de submeter sua solução pelo Edmodo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### teste 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Aqui tem:\n",
    "\n",
    "- Geração WordCloud\n",
    "- Modelagem de Tópicos\n",
    "- Parcelas de comprimento de perguntas\n",
    "- Estimativa de estrutura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import sklearn\n",
    "# assert sklearn.__version__ == '0.18' # Make sure we are in the modern age\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv('S08_question_answer_pairs.txt', encoding='utf-8', delimiter='\\t', quotechar='\"')\n",
    "raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "WordCloud!\n",
    "\n",
    "Uma nuvem da palavra de todo o presente do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text = ' '.join(raw['ArticleTitle'])\n",
    "cloud = WordCloud(background_color='white', width=1920, height=1080).generate(text)\n",
    "plt.figure(figsize=(32, 18))\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud)\n",
    "plt.savefig('questions_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Modelos de Tópicos\n",
    "\n",
    "Como em qualquer análise que se respeite sobre dados de texto não rotulados, realizamos aqui alguns modelos de tópicos com a Fatoração de Matriz Não Negativa nas perguntas.\n",
    "\n",
    "Isso nos permite saber sobre os diferentes tipos de categorias de piadas de perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw['Question'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw['Question'].values.astype('U').nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw['Question'].values.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some defaults\n",
    "max_features=1715\n",
    "max_df=0.95,  \n",
    "min_df=2,\n",
    "max_features=1715,\n",
    "stop_words='english'\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# document-term matrix A\n",
    "vectorized = CountVectorizer(max_features=1715, max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "a = vectorized.fit_transform(raw.Question)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(init=\"nndsvd\",\n",
    "            n_components=10,\n",
    "            max_iter=200)\n",
    "\n",
    "# Get W and H, the factors\n",
    "W = model.fit_transform(a)\n",
    "H = model.components_\n",
    "\n",
    "print(\"W:\", W.shape)\n",
    "print(\"H:\", H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Obter a lista de todos os termos cujos índices correspondem às colunas da matriz de termo do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = vectorized\n",
    "\n",
    "terms = [\"\"] * len(vectorizer.vocabulary_)\n",
    "for term in vectorizer.vocabulary_.keys():\n",
    "    terms[vectorizer.vocabulary_[term]] = term\n",
    "    \n",
    "# Have a look that some of the terms\n",
    "terms[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for topic_index in range(H.shape[0]):  # H.shape[0] is k\n",
    "    top_indices = np.argsort(H[topic_index,:])[::-1][0:10]\n",
    "    term_ranking = [terms[i] for i in top_indices]\n",
    "    print(\"Topic {}: {}\".format(topic_index, \", \".join(term_ranking)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Podemos ver alguns tipos populares de piadas de perguntas. Para citar alguns que ouvi:\n",
    "\n",
    "- Atravesse a rua\n",
    "- Trocar uma lâmpada\n",
    "- Qual é a diferença b / w A e B\n",
    "- Minha piada favorita\n",
    "\n",
    "Análise de sentimentos\n",
    "\n",
    "Atribuímos pontuações de sentimento às perguntas e respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_polarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "get_subjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "raw['q_polarity'] = raw.Question.apply(get_polarity)\n",
    "raw['a_polarity'] = raw.Answer.apply(get_polarity)\n",
    "raw['q_subjectivity'] = raw.Question.apply(get_subjectivity)\n",
    "raw['a_subjectivity'] = raw.Answer.apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "sns.distplot(raw.q_polarity , label='Question Polarity')\n",
    "sns.distplot(raw.q_subjectivity , label='Question Subjectivity')\n",
    "sns.distplot(raw.a_polarity , label='Answer Polarity')\n",
    "sns.distplot(raw.a_subjectivity , label='Answer Subjectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Talvez seja uma piada é bom se o sentimento muda ao responder a pergunta? Infelizmente não há como responder a isso por causa da falta de um recurso de pontuação / upvote de piada nesta versão do conjunto de dados.\n",
    "\n",
    "Sobre as próprias piadas\n",
    "\n",
    "O que podemos dizer das próprias piadas? Vamos dar uma olhada no comprimento primeiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daf = raw.loc[raw.Answer.str.len() < 150]  # There appear to be some outliers in the dataset\n",
    "sns.distplot(daf.Question.str.len(), label='Question Length')\n",
    "sns.distplot(daf.Answer.str.len(), label='Answer Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw.loc[raw.Answer.str.len() > 400].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sabemos que as respostas são geralmente menores que as perguntas. Existem perguntas cujas respostas são mais curtas do que elas? E o reverso?\n",
    "\n",
    "Resultados semelhantes são válidos. Uma comparação melhor do comprimento da pergunta versus a duração da resposta seria um gráfico de dispersão. Até agora temos plotado a diferença, mas o que isso perde são os comprimentos exatos de Q e A. 500 - 550 é o mesmo que 10 - 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ql, al = 'Question Length', 'Answer Length'\n",
    "raw[ql] = raw.Question.str.len()\n",
    "raw[al] = raw.Answer.str.len()\n",
    "daf = raw.loc[raw[al] < 250]\n",
    "sns.jointplot(x=ql, y=al, data=daf, kind='kde', space=0, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('S08_question_answer_pairs.txt', encoding='utf-8', delimiter='\\t', quotechar='\"')\n",
    "raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Non-ASCII characters\n",
    "def remove_non_ascii_1(raw):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### teste de varios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleTitle = str(raw['ArticleTitle'][i])\n",
    "\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitlesentence_tokens = nltk.sent_tokenize(ArticleTitle)\n",
    "    ArticleTitleword_tokens = nltk.word_tokenize(ArticleTitle)\n",
    "\n",
    "    [ArticleTitlesentence_tokens[:2], ArticleTitleword_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = str(raw['Question'][i])\n",
    "\n",
    "for i in range(0, 1715):\n",
    "    Questionsentence_tokens = nltk.sent_tokenize(Question)\n",
    "    Questionword_tokens = nltk.word_tokenize(Question)\n",
    "\n",
    "    [Questionsentence_tokens[:2], Questionword_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = str(raw['Answer'][i])\n",
    "\n",
    "for i in range(0, 1715):\n",
    "    Answersentence_tokens = nltk.sent_tokenize(Answer)\n",
    "    Answerword_tokens = nltk.word_tokenize(Answer)\n",
    "\n",
    "    [Answersentence_tokens[:2], Answerword_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DifficultyFromQuestioner = str(raw['DifficultyFromQuestioner'][i])\n",
    "\n",
    "for i in range(0, 1715):\n",
    "    DifficultyFromQuestionersentence_tokens = nltk.sent_tokenize(DifficultyFromQuestioner)\n",
    "    DifficultyFromQuestionerword_tokens = nltk.word_tokenize(DifficultyFromQuestioner)\n",
    "\n",
    "    [DifficultyFromQuestionersentence_tokens[:2], DifficultyFromQuestionerword_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DifficultyFromAnswerer = str(raw['DifficultyFromAnswerer'][i])\n",
    "\n",
    "for i in range(0, 1715):\n",
    "    DifficultyFromAnswerersentence_tokens = nltk.sent_tokenize(DifficultyFromAnswerer)\n",
    "    DifficultyFromAnswererword_tokens = nltk.word_tokenize(DifficultyFromAnswerer)\n",
    "\n",
    "    [DifficultyFromAnswerersentence_tokens[:2], DifficultyFromAnswererword_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleFile = str(raw['ArticleFile'][i])\n",
    "\n",
    "for i in range(0, 1715):\n",
    "    ArticleFilesentence_tokens = nltk.sent_tokenize(ArticleFile)\n",
    "    ArticleFileword_tokens = nltk.word_tokenize(ArticleFile)\n",
    "\n",
    "    [ArticleFilesentence_tokens[:2], ArticleFileword_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### teste de varios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleTitlecorpus = []\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitle = str(raw['ArticleTitle'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    ArticleTitlecorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questioncorpus = []\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitle = str(raw['Question'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    Questioncorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answercorpus = []\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitle = str(raw['Answer'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    Answercorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DifficultyFromQuestionercorpus = []\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitle = str(raw['DifficultyFromQuestioner'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    DifficultyFromQuestionercorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DifficultyFromAnswerercorpus = []\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitle = str(raw['DifficultyFromAnswerer'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    DifficultyFromAnswerercorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleFilecorpus = []\n",
    "for i in range(0, 1715):\n",
    "    ArticleTitle = str(raw['ArticleFile'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    ArticleFilecorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def lem_tokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def lem_normalize(text):\n",
    "    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = ('hello', 'hi', 'greetings', 'sup', 'what\\'s up', 'hey',)\n",
    "GREETING_RESPONSES = ['hi', 'hey', '*nods*', 'hi there', 'hello', 'I am glad! You are talking to me']\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    Answersentence_tokens.append(user_response)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=lem_normalize, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform(sentence_tokens)\n",
    "    \n",
    "    values = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = values.argsort()[0][-2]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if req_tfidf is 0:\n",
    "        robo_response = '{} Sorry, I don\\'t understand you'.format(robo_response)\n",
    "    else:\n",
    "        robo_response = robo_response + Questionsentence_tokens[idx]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "print('BOT: My name is Robo, I will answer your questions about chatbots. If you want to exit, type Bye')\n",
    "\n",
    "while flag:\n",
    "    \n",
    "    while True:\n",
    "        os.system('clear')\n",
    "        per_usr = input('[bot] Diga lá!')\n",
    "        if per_usr.lower() in [p.lower() for p in Questionsentence_tokens]:\n",
    "            for idx, pergs in enumerate(Questionsentence_tokens):\n",
    "                if per_usr.lower() == pergs.lower():\n",
    "                    print(Answersentence_tokens[idx])\n",
    "                    break\n",
    "  \n",
    "        input('Press ENTER to continue:')\n",
    "    else:\n",
    "        flag = False\n",
    "        print('BOT: bye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### continuação normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_tokens = nltk.sent_tokenize(raw)\n",
    "#word_tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "#str(raw[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 1715):\n",
    "    review = str(raw['ArticleTitle'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "    \n",
    "    sentence_tokens = nltk.sent_tokenize(raw)\n",
    "    word_tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "    [sentence_tokens[:2], word_tokens[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def lem_tokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def lem_normalize(text):\n",
    "    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = ('hello', 'hi', 'greetings', 'sup', 'what\\'s up', 'hey',)\n",
    "GREETING_RESPONSES = ['hi', 'hey', '*nods*', 'hi there', 'hello', 'I am glad! You are talking to me']\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    sentence_tokens.append(user_response)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=lem_normalize, stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform(sentence_tokens)\n",
    "    \n",
    "    values = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = values.argsort()[0][-2]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if req_tfidf is 0:\n",
    "        robo_response = '{} Sorry, I don\\'t understand you'.format(robo_response)\n",
    "    else:\n",
    "        robo_response = robo_response + sentence_tokens[idx]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = True\n",
    "print('BOT: My name is Robo, I will answer your questions about chatbots. If you want to exit, type Bye')\n",
    "\n",
    "interactions = [\n",
    "    'hi',\n",
    "    'what is chatbot?',\n",
    "    'describe its design, please',\n",
    "    'what about alan turing?',\n",
    "    'and facebook?',\n",
    "    'sounds awesome',\n",
    "    'bye',\n",
    "]\n",
    "\n",
    "while flag:\n",
    "    user_response = interactions.pop(0)\n",
    "    print('USER: {}'.format(user_response))\n",
    "    if user_response is not 'bye':\n",
    "        if user_response is 'thanks' or user_response is 'thank you':\n",
    "            flag = False\n",
    "            print('BOT: You are welcome...')\n",
    "        else:\n",
    "            if greeting(user_response) is not None:\n",
    "                print('ROBO: {}'.format(greeting(user_response)))\n",
    "            else:\n",
    "                print('ROBO: ', end='')\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print('BOT: bye!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Teste 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Exemplo dado em sala do meetup que funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# chat bot\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lista de perguntas\n",
    "perguntas = ['oi','Vai chover hoje?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    os.system('clear')\n",
    "    per_usr = input('[bot] Diga lá!')\n",
    "    if per_usr.lower() in [p.lower() for p in perguntas]:\n",
    "        for idx, pergs in enumerate(perguntas):\n",
    "            if per_usr.lower() == pergs.lower():\n",
    "                print(respostas[idx])\n",
    "                break\n",
    "    else:\n",
    "        print('Não entendi ?')\n",
    "        ens_usr = input('Quer que eu aprenda?')\n",
    "        if ens_usr.lower() == 'sim':\n",
    "            perguntas.append(per_usr)\n",
    "            resp_usr = input('O que devo responder:')\n",
    "            respostas.append(resp_usr)\n",
    "\n",
    "    input('Press ENTER to continue:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "#dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "# Importing the dataset\n",
    "cols = [\"ArticleTitle\",\"Question\",\"Answer\"]\n",
    "dataset = pd.read_csv('S08_question_answer_pairs.txt', delimiter = '\\t', usecols=cols,\n",
    "                      quoting = 3, error_bad_lines=False, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Non-ASCII characters\n",
    "def remove_non_ascii_1(dataset):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 1715):\n",
    "    review = str(dataset['requisicoes'][i])\n",
    "    #review = re.sub('[^a-zA-Z]', ' ', dataset['Question'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat] = le.fit_transform(df[cat].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "accuracies.mean()\n",
    "accuracies.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.kaggle.com/cristianossd/nlp-chatbot-using-nltk\n",
    "\n",
    "https://www.kaggle.com/quora/question-pairs-dataset\n",
    "\n",
    "https://www.kaggle.com/quora/question-pairs-dataset\n",
    "\n",
    "https://www.kaggle.com/stanfordu/stanford-question-answering-dataset\n",
    "\n",
    "https://www.kaggle.com/jiriroz/qa-jokes\n",
    "\n",
    "https://www.kaggle.com/bharathsh/stanford-q-a-json-to-clean-dataframe\n",
    "\n",
    "https://www.kaggle.com/onesz19/scout-script-about-the-jokes\n",
    "\n",
    "https://www.kaggle.com/karanabhishek/chatbot-try"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
